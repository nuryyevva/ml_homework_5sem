{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qjl6x6og3uXH"
   },
   "source": [
    "# HW 2 - Разложение матриц градиентным методом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sv79QFb_-oNZ"
   },
   "source": [
    "Цель задания: В ходе реализации [разложения Таккера](https://proceedings.neurips.cc/paper/2018/file/45a766fa266ea2ebeb6680fa139d2a3d-Paper.pdf) градиентным методом освоить pyTorch и реализовать подходы оптимизации параметров модели (в отсутствии готовых решений)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HUSrylpBwYn"
   },
   "source": [
    "[Более-менее внятное описание алгоритма канонического разложения](https://www.alexejgossmann.com/tensor_decomposition_tucker/) - само аналитическое разложение вам реализовывать НЕ НУЖНО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "P1PuoBtG7iw7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7352fd752c90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.linalg import svd, matrix_rank, pinv, inv\n",
    "from scipy.linalg import eigh, eig\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import nn\n",
    "\n",
    "import tensorly as tl\n",
    "tl.set_backend('pytorch')\n",
    "from tensorly.decomposition import tucker\n",
    "from tensorly.tucker_tensor import tucker_to_tensor\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LfhKpuX7htE"
   },
   "source": [
    "## 1 Создайте 3х мерный тензор\n",
    "Размер тензора не меньше 100 по каждой из размерностей.\n",
    "\n",
    "Заполните случайными целыми числами в диапазоне от 0 до 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ap1Ozn7P8-Yj"
   },
   "source": [
    "Примечание: разложение будет корректно работать со случайным тензором, только если изначально создавать случайные ядро и матрицы, а потом по ним формировать тензор. Работайте с типом *torch.Tensor.double*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5SzHzteOROQQ"
   },
   "outputs": [],
   "source": [
    "# Создадим тензор: размер тензора и r задаётся\n",
    "def get_tensor(size=(100,200,150), r=(10, 20, 15)):\n",
    "    # data - тензор с заданной размерностью\n",
    "    # U - список матриц\n",
    "    # G - ядро разложения\n",
    "    G = torch.randint(10, (r[0], r[1], r[2]), requires_grad=True, dtype=torch.double)\n",
    "    U = [torch.randint(10, (size[i], r[i]), requires_grad=True, dtype=torch.double) for i in range(3)]\n",
    "\n",
    "    data = G\n",
    "    \n",
    "    data = torch.tensordot(U[0], data, dims=([1], [0]))\n",
    "    \n",
    "    data = torch.tensordot(U[1], data.permute([1, 0, 2]), dims=([1], [0]))\n",
    "    data = data.permute([1, 0, 2])\n",
    "    \n",
    "    data = torch.tensordot(U[2], data.permute([2, 1, 0]), dims=([1], [0]))\n",
    "    data = data.permute([2, 1, 0])\n",
    "\n",
    "    return data, U, G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFuFlp2n78Tz"
   },
   "source": [
    "Сгенерируйте тензор и добавьте к нему случайный шум с размерностью *1e-2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FnUbbsYSdrsw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 200, 300]),\n",
       " [torch.Size([100, 10]), torch.Size([200, 20]), torch.Size([300, 30])],\n",
       " torch.Size([10, 20, 30]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = (100, 200, 300)\n",
    "r = (10, 20, 30)\n",
    "\n",
    "data, U, G = get_tensor(size, r)\n",
    "data.shape, [u.shape for u in U], G.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "N70Xy_6u9RFa"
   },
   "outputs": [],
   "source": [
    "noise = torch.randint(10, (data.shape)) * 1e-2\n",
    "data += noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kp75_Ad29RL5"
   },
   "source": [
    "Вопрос:\n",
    "Почему задание не имеет смысла для полностью случайного тензора и зачем добавлять шум? *не отвечать нельзя*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VLMaT5wyE11"
   },
   "source": [
    "Ответ:\n",
    "+ Разложение Такера ищет простые структуры в данных. Если данные случайные и структуры нет, разложение бесполезно, результат будет почти таким же, как и исходные данные.\n",
    "+ Градиентный спуск плохо работает с очень сложными данными, у которых множество локальных минимумов. А случайные данные именно такие.\n",
    "+ Добавление шума помогает найти глобальный минимум среди множества других."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzninpMYD_hd"
   },
   "source": [
    "## 2 Реализуйте метод для восстановления тензора по разложению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YDTx9ZbYD-_S"
   },
   "outputs": [],
   "source": [
    "# Функция, восстанавливающая тензор по ядру и матрицам\n",
    "def repair_tensor(G_, U):\n",
    "    # data - восстановленный тензор из матриц и ядра\n",
    "    # U - список матриц\n",
    "    # G_ - ядро разложения\n",
    "    data = G_\n",
    "    for i, U_i in enumerate(U):\n",
    "        permutation = list(range(data.dim()))\n",
    "        permutation[0], permutation[i] = permutation[i], permutation[0]\n",
    "        data = torch.tensordot(U_i, data.permute(permutation), dims=([1], [0])).permute(permutation)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KvEKNuTvIIfp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\n",
      "data: torch.Size([150, 200, 250])\n",
      "data_p: torch.Size([150, 200, 250])\n",
      "\n",
      "MSE: 0.0\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "data_test, U_test, G_test = get_tensor()\n",
    "reconstructed_data = repair_tensor(G_test, U_test)\n",
    "print(torch.allclose(data_test, reconstructed_data))\n",
    "\n",
    "r = (15, 20, 25)\n",
    "_, U, G = get_tensor(size=(150, 200, 250), r=r)\n",
    "\n",
    "data = repair_tensor(G, U)\n",
    "print('\\ndata:', data.shape)\n",
    "\n",
    "data_p = tucker_to_tensor((G, U))\n",
    "print('data_p:', data_p.shape)\n",
    "\n",
    "print('\\nMSE:', mean_squared_error(data.detach().numpy().flatten(), data_p.detach().numpy().flatten()))\n",
    "print((data==data_p).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKqzxtaE-F16"
   },
   "source": [
    "## 3 Сделайте разложение библиотечным методом\n",
    "Пакет можете брать любой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Hlp4Jh3--fKh"
   },
   "outputs": [],
   "source": [
    "G_bib, U_bib = tucker(data, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMw1x8w8-lsh"
   },
   "source": [
    "Не забудьте померить ошибку разложения по метрике MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HWkdb7Ip-mL3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.1962010929834375e-17\n"
     ]
    }
   ],
   "source": [
    "data_bib = tucker_to_tensor((G_bib, U_bib))\n",
    "print(\"MSE:\", mean_squared_error(data.detach().numpy().flatten(), data_bib.detach().numpy().flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibOgeEgfD1wm"
   },
   "source": [
    "## 4 Реализуйте разложение градиентным методом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GstBYmiBF7A6"
   },
   "source": [
    "### 4.1 Реализуйте *optimizer*\n",
    "Можно взять из исходников *PyTorch* и отнаследоваться от *torch.optim.optimizer*.\n",
    "Используйте квадратичный *Loss*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Mxrtt60hF6xb"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.optim import SGD\n",
    "\n",
    "\n",
    "class Opt(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        self.lr = lr\n",
    "        defaults = dict(lr=self.lr)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = param.grad.data\n",
    "                if torch.isnan(grad).any() or torch.isinf(grad).any():\n",
    "                    print(\"Warning: NaN or Inf gradient detected for parameter:\", param)\n",
    "                    continue  \n",
    "\n",
    "                param.data.add_(-group['lr'], grad)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GSolH5dEJba"
   },
   "source": [
    "### 4.2 Реализуйте цикл оптимизации параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6UWpuERFTn8"
   },
   "source": [
    "Стоит параметры оптимизировать сразу на GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CgPaeQ7XEJnD"
   },
   "outputs": [],
   "source": [
    "class TuckerModel(nn.Module):\n",
    "    def __init__(self, factor_matrices, core_tensor):\n",
    "        super().__init__()\n",
    "        self.factor_matrices = nn.ParameterList([nn.Parameter(u) for u in factor_matrices])\n",
    "        self.core_tensor = nn.Parameter(core_tensor)\n",
    "\n",
    "    def forward(self):\n",
    "        return repair_tensor(self.core_tensor, self.factor_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = (10, 15, 25)\n",
    "size = (150, 200, 250)\n",
    "\n",
    "data_train, U, G = get_tensor(size = size, r = r)\n",
    "noise = torch.randint(10, (data_train.shape))\n",
    "data_train += noise\n",
    "data_train = data_train.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_train, U, G, num_epochs=100, verbose=True, log_frequency=10, lr=1e-10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    data_train = data_train.to(device)\n",
    "    U = [u.to(device) for u in U]\n",
    "    G = G.to(device)\n",
    "\n",
    "    model = TuckerModel(U, G).to(device) \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = Opt(model.parameters(), lr = lr)\n",
    "\n",
    "    loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        data_predicted = model.forward()\n",
    "        loss = criterion(data_predicted, data_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        if verbose and epoch % log_frequency == 0:\n",
    "            print(f\"--------Epoch {epoch}--------\")\n",
    "            print(f\"Loss {loss.item()}\")\n",
    "            print(f\"-----------------------\\n\")\n",
    "\n",
    "    return model, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "--------Epoch 0--------\n",
      "Loss 8.57948525944512\n",
      "-----------------------\n",
      "\n",
      "--------Epoch 10--------\n",
      "Loss 8.535723800854878\n",
      "-----------------------\n",
      "\n",
      "--------Epoch 20--------\n",
      "Loss 8.501861575731894\n",
      "-----------------------\n",
      "\n",
      "--------Epoch 30--------\n",
      "Loss 8.4746523964\n",
      "-----------------------\n",
      "\n",
      "--------Epoch 40--------\n",
      "Loss 8.452184111308302\n",
      "-----------------------\n",
      "\n",
      "--------Epoch 50--------\n",
      "Loss 8.433267918007859\n",
      "-----------------------\n",
      "\n",
      "--------Epoch 60--------\n",
      "Loss 8.41712106214531\n",
      "-----------------------\n",
      "\n",
      "--------Epoch 70--------\n",
      "Loss 8.403198826092229\n",
      "-----------------------\n",
      "\n",
      "--------Epoch 80--------\n",
      "Loss 8.391103235147732\n",
      "-----------------------\n",
      "\n",
      "--------Epoch 90--------\n",
      "Loss 8.38053174455751\n",
      "-----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, loss_history = train_model(data_train, U, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Za8JKgR-Falk"
   },
   "source": [
    "## 5 Приведите сравнение скорости работы и ошибки восстановления методом из пакета и реализованного градиентного\n",
    "Сравнение может считаться ± объективным с размером выборки от 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mOGKW9RHFa5D"
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "\n",
    "for _ in range(20):\n",
    "    data_train, U, G = get_tensor(size=size, r=r)\n",
    "    noise = torch.randint(10, (data_train.shape))*1e-2\n",
    "    data_train += noise\n",
    "    data_train = data_train.detach()\n",
    "    train_data.append((data_train, U, G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "losses_grad = []\n",
    "\n",
    "time1 = time.time()\n",
    "for i in range(20):\n",
    "    model, loss_history = train_model(*train_data[i], verbose=False, num_epochs=30)\n",
    "    losses_grad.append(loss_history[-1])\n",
    "time2 = time.time() - time1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0011473455898549391 \n",
      "time: 95.94947\n"
     ]
    }
   ],
   "source": [
    "print(\"loss:\", sum(losses_grad) / len(losses_grad), \"\\ntime:\", np.round(time2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_lib = []\n",
    "\n",
    "time1 = time.time()\n",
    "for i in range(20):\n",
    "    data, U, G = train_data[i]\n",
    "    data_p = tucker_to_tensor((G, U))\n",
    "    losses_lib.append(mean_squared_error(data.detach().numpy().flatten(), data_p.detach().numpy().flatten()))\n",
    "time2 = time.time() - time1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0011295887886070004 \n",
      "time: 1.985\n"
     ]
    }
   ],
   "source": [
    "print(\"loss:\", sum(losses_lib) / len(losses_lib), \"\\ntime:\", np.round(time2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
